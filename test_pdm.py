# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'

# %%
from pdm_utils import *
from pdmdb_generator import *

from pdm.pdm_train  import *
from pdm.pdm_monitoring import *

import pandas as pd
import numpy as np
from sklearn import metrics
from sklearn.model_selection import train_test_split
from memory_profiler import memory_usage

from  IPython import display
from matplotlib import pyplot as plt

import seaborn as sns

# %% [markdown]
# ############ Generate Data #####################
# ################################################

# %%

# Instantiate the generator
generator=pdmdb_generator(nbitems=50,l=8, lp=8)

# Stores all the chronicle sequences generated by the generator 
chro_sequences = generator.generate(nb=100, l=25, npat=30, th=0.5)

# Stores all the sequences taken from each chronicle sequence of the variable chro_sequences
sequences=[s.seq for s in chro_sequences]

# The disturbed chronicles to be used for generating disturbed sequences
disturbed_chronicles = []

for c in generator.patterns:
    # Probability of: [1:removing items, 2:modifying constraints, 3:adding items]
    prob = [0,1,0]  # Set probability of modifying constraints to 1 and 0 for removing/adding
    # Generate disturbed a chronicle from chronicle c with probability prob
    disturbed_chronicle = generator.pattern_generator.generate_similar(c,prob)
    disturbed_chronicles.append(disturbed_chronicle)

# Stores all the disturbed chronicle sequences generated by the generator 
disturbed_chro_sequences = generator.generate(nb=100, l=25, npat=30, th=0.5, patterns=disturbed_chronicles)

# Stores all the sequences taken from each chronicle sequence of the variable disturbed_chro_sequences
disturbed_sequences=[s.seq for s in disturbed_chro_sequences]

# Removes from sequences and disturbed_sequences all events equal to -1
sequences=filterseq(sequences)
disturbed_sequences=filterseq(disturbed_sequences)
# %% [markdown]
# ############ Saves Data #####################
# ################################################

# %%

# Serialize all variables
serialization2(sequences,"sequences.txt")
serialization2(disturbed_sequences,"disturbed_sequences.txt")

# %%
print("====================\nChronicle DB generation\n====================\n")
# vocabulary size: 50 // mean length of the sequences: 8 // pattern lenght: 8
generator=chrodb_generator(nbitems=50,l=8, lp=8)

# pat_gen=chrodb_generator.pattern_generator

# sequences: stores all the chronicle sequences generated by the generator (100 sequences and 30 patterns)
# TODO: Why the the mean lenght of the sequence (l) is 25??
sequences = generator.generate(nb=100, l=25, npat=30, th=0.5)

print("======== PATTERNS =======")

# TODO: figure out difference between objects of type Chronicle stored in DB_c and DB_ch
# DB_c: stores the (30) chronicles generated by seq_generation.chronicle_generator.py
DB_c=[ch for ch in generator.all_patterns()]

# DB_ch: stores the chronicles generated by recognition.chronicle_recognition.py
DB_ch=[affectation(ch) for ch in generator.all_patterns()]

# DB_seq: stores all the sequences taken from each chronicle sequence of the variable sequences
DB_seq=[s.seq for s in sequences]
    
# Removes from DB_seq all events equal to -1
DB_seq=filterseq(DB_seq)

# DB_seq_label: stores the labels for each event in DB_seq. All labels in DB_seq_label are equal to 1
DB_seq_label=makelabel(DB_seq)

assert (len(DB_seq)==len(DB_seq_label))

print("======== make_noise in CH DB =======")
per=.3

# DB_c is split into 2 parts according to the percentage per
# c: is non disturbed data
# ch: is disturbed data
c,ch=split_db_ch(DB_c,per=per)

print("======== p sequence generation =======") 
# TODO: This is the generation of disturbed data??? The parameter pert in generator.generate is not really used,
# so what is the difference between p_seq0 and p_seq1???

# p_se0: stores all the chronicle sequences generated by the generator (300 sequences and len(ch) patterns). This variable is of the same type of the variable sequences defined above
p_se0=generator.generate(nb=300, l=15, npat=len(ch), th=.5,patterns=ch,pert=0)
# p_seq0: stores all the sequences taken from each chronicle sequence of the variable p_se0
p_seq0=[s.seq for s in p_se0]

# p_se1: stores all the chronicle sequences generated by the generator (200 sequences and len(ch) patterns). This variable is of the same type of the variables sequences and p_se0 defined above
# TODO: Why the the mean lenght of the sequence (l) is 25 for p_se1 and 15 for p_se0??
p_se1=generator.generate(nb=200, l=25, npat=len(ch), th=.5,patterns=ch,pert=1)
# p_seq1: stores all the sequences taken from each chronicle sequence of the variable p_se1
p_seq1=[s.seq  for s in p_se1]

# Removes from p_seq0 and p_seq1 all events equal to -1
p_seq0=filterseq(p_seq0)
p_seq1=filterseq(p_seq1)

# Serialize all variables
serialization2(p_seq0,"p_seq0.txt")
serialization(p_seq1,"p_seq1.txt")
serialization2(DB_seq,"DB_seq.txt")
serialization(DB_seq_label,"DB_seq_label.txt")
serialization(DB_ch,"DB_ch.txt")

# %%
############### generate train seq for LSTM ######################
trainseq=[]
m=[25]
for i,ch in enumerate(DB_c[:]):
    for k in m: 
        ps=generator.generate(nb=5, l=k, npat=len([ch]), th=.5,patterns=[ch],pert=-2)
        # pseq=filterseq([s.seq for s in ps])
        pseq=[s.seq for s in ps]
        pseq=filterseq(pseq)
        # print(len(ps))
        if len(pseq)!=0:
            trainseq.extend(pseq)


# %%

serialization(trainseq,"test.txt")
serialization2(trainseq,"test2.txt")

# %%
# read_text_file("p_seq1.txt")

# %%
# from pdm.pdm_train import *
print("####### Start of traning ########")

h=train(p_seq1)
saveobj("history.hist",h)

print("####### End of traning ########")

# %%
##load model and parameter
# from keras.models import load_model

model = load_model('./pdm/output_files/model.h5')
with open('./pdm/output_files/config.dictionary', 'rb') as config_file:
    param = pickle.load(config_file)
# %%
# summarize history for loss
h1=loadobj("history.hist")
history=h1

print("*********************************")
print("*********************************")
print("*********************************")

plt.plot(history.history['act_output_acc'])
plt.plot(history.history['val_act_output_acc'])
plt.title('Model accuracy for events')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='lower right')
plt.show()
# %%

plt.plot(history.history['time_output_acc'])
plt.plot(history.history['val_time_output_acc'])
plt.title('Model accuracy for time')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
# %%

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# %%

plt.plot(history.history['act_output_loss'])
plt.plot(history.history['val_act_output_loss'])
plt.title('Model loss for events')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
# %%

plt.plot(history.history['time_output_loss'])
plt.plot(history.history['val_time_output_loss'])
plt.title('Model loss for time')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# %%

plt.plot(history.history['time_output_mean_squared_error'])
plt.plot(history.history['val_time_output_mean_squared_error'])
plt.title('Model mse for time')
plt.ylabel('mse')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# %%

print("======== SEQUENCES =======")
seq_df=pd.DataFrame({'sequence':[], 'label':[]})
#DB_seq=[]
#DB_seq_label=[]
seq_df.sequence=pd.Series(DB_seq[:]+p_seq0[:]+p_seq1[:])
nbseq=len(seq_df.sequence)

seq_df.label=pd.Series(len(DB_seq[:])*[1]+len(p_seq0[:])*[1]+len(p_seq1[:])*[0])
print(seq_df.shape)
print(seq_df.head(10))

# %%

def LSTMpred(seqs):
    start_time = time.perf_counter()
    SS=[anomalydetect([seq],param,model) for seq in [filterseq(seqs)[0]]]
    end_time=time.perf_counter()
    le=end_time-start_time
    return SS,le 

SS,le= LSTMpred(list(seq_df.sequence))
print(SS)

# %%
# ############### generate test data ######################
# nbitems=50 #nb vocab
# seqlen=15 #len ch
# ch=DB_c[:]
# nbc=len(ch)
# generator=chrodb_generator(nbitems=50,l=25, lp=15)
# DB_seq=generator.generate(nb=50, l=25, npat=len(ch), th=.5,patterns=ch,pert=-1)
# p_seq0=generator.generate(nb=20, l=25, npat=len(ch), th=.5,patterns=ch,pert=0)
# p_seq1=generator.generate(nb=20, l=25, npat=len(ch), th=.5,patterns=ch,pert=1)

# #p_seq0=filterseq([s.seq for s in p_seq0])
# #p_seq1=filterseq([s.seq for s in p_seq1])
# #DB_seq=filterseq([s.seq for s in DB_seq])
# DB_seq_label=len(DB_seq)*[1]
# #trainseq.extend(pseq)
# print(len(DB_seq))
# print(len(p_seq0))
# print(len(p_seq1))

# %% [markdown]
# ############### Data Frame #################
# ###########################################
# %%
nbitems=50 #nb vocab
seqlen=10 #len ch
# p_seq0=[]
# p_seq1=[]
DB_seq=trainseq[:]

DB_seq[:]+p_seq0[:]+p_seq1[:]

times = []
events=[]

for i,seq in enumerate(DB_seq): #the rows are "ChID,sequence,TC"
    if len(seq)==0:
        continue
    for t,e in seq:
        events.append(e)
        times.append(t)

    # lines.append(evnts)
    # timeseqs.append(times)

print(len(events))
print(len(times))
# seq_df=pd.DataFrame({'event':events, 'time':times})
seq_df=pd.DataFrame({'event':[], 'time':[]})
s=seq_df.event=pd.Series(events)
seq_df.time=pd.Series(times)

# seq_df.sequence=pd.Series(DB_seq[:]+p_seq0[:]+p_seq1[:])

s.value_counts()
# seq_df.plot(kind='bar')

# %%

print("======== SEQUENCES =======")
seq_df=pd.DataFrame({'sequence':[], 'label':[]})
#DB_seq=[]
#DB_seq_label=[]
seq_df.sequence=pd.Series(DB_seq[:]+p_seq0[:]+p_seq1[:])
nbseq=len(seq_df.sequence)

seq_df.label=pd.Series(len(DB_seq[:])*[1]+len(p_seq0[:])*[1]+len(p_seq1[:])*[0])
print(seq_df.shape)
print(seq_df.head(10))

#for e in seq_df.sequence:
#    print(e)

# %%

dataset = seq_df.copy()

print("head:")
train_dataset = dataset.sample(frac=0.8,random_state=0)
train_dataset.head()
# train_dataset.index
train_dataset.index
# train_dataset.describe()
# train_dataset['sequence']
# train_dataset.iloc[3]

# train_stats = train_dataset.describe()
# train_stats.pop("sequence")
# train_stats = train_stats.transpose()
# train_stats


# %%

import seaborn as sns
sns.pairplot(train_dataset[["sequence", "label"]], diag_kind="kde")


# %%

print("tail:")
test_dataset = dataset.drop(train_dataset.index)
test_dataset.tail()

# %%
# ############### Training phase #################
# ################################################


# %%
# ############### Plot phase #################
# ################################################

# %%
print(h)
# plotter = tfdocs.plots.HistoryPlotter(metric = 'categorical_crossentropy', smoothing_std=10)
# plotter.plot(h)
# plt.ylim([0.5, 0.7])
# %%
%load_ext tensorboard
import tensorflow as tf 

print("TF VERSION: ",tf.__version__)

%tensorboard --logdir LSTM/output_files/summary

# %% [markdown]
# ############ LSTM ########################

# %%
##load model and parameter
model = load_model('./LSTM/output_files/model.h5')
with open('./LSTM/output_files/config.dictionary', 'rb') as config_file:
    param = pickle.load(config_file)
#seqs=read_text_file("./LSTM/input_data/testseqs.txt")
#trainseq=[ deserialization(line) for line in seqs ]
def LSTMpred(seqs):
    start_time = time.perf_counter()
    SS=[anomalydetect([seq],param,model) for seq in [filterseq(seqs)[0]]]
    end_time=time.perf_counter()
    le=end_time-start_time
    return SS,le
#SS,le= LSTMpred(trainseq)
#SS,le= LSTMpred(list(seq_df.sequence))
#lmu=np.mean(memory_usage((LSTMpred,(list(seq_df.sequence),))))

#print(SS)


# %%
j=0
nbc=1
pert=.3
for i in range(15,150,15):
    nbitems=np.mean([len(s) for s in list(seq_df.sequence)[j:i]])
    SS,le= LSTMpred(list(seq_df.sequence)[j:i])
    # JGT: lmu=np.mean(memory_usage((LSTMpred,(list(seq_df.sequence),))))
    print(SS)
    seuil=0.6
    #ss,fl,f,seuil=learn_threshold(SS,seq_df.label[j:i])
    y_pred=decision(SS,seuil)
    print(y_pred)

    #assert(len(seq_df)==len(y_pred))
    # Display the confusion matrix
    #print(metrics.confusion_matrix(seq_df.label[j:i], y_pred))
    # Calculate the classification rate of this classifier
    #p=metrics.accuracy_score(seq_df.label[j:i], y_pred)
    #r=metrics.recall_score(seq_df.label[j:i], y_pred, average='macro')
    #f1=metrics.f1_score(seq_df.label[j:i], y_pred, average='macro')
    #p1,r1,f,_=metrics.precision_recall_fscore_support(seq_df.label[j:i], y_pred, average='micro')
    #print(f1,r,p)
    # JGT: savescore("LSTM3.csv",p,r,f1,le,lmu)

    p1,r1,f,_=metrics.precision_recall_fscore_support([seq_df.label[j]], y_pred, average='micro')
    savescore("LSTM.csv",p1,r1,f,le,0)

    j=i


# %%
j=0
nbc=3
pert=.3
for s in trainseq[8:]:
    #s=filterseq([s])[0]
    nbitems=len(s)
    SS,le= LSTMpred([s])
    # lmu=np.mean(memory_usage((LSTMpred,([s],))))
    print(SS)
    ss,fl,f,seuil=learn_threshold(SS,[seq_df.label[j]])
    y_pred=decision(SS,seuil)
    print(y_pred)

    #assert(len(seq_df)==len(y_pred))
    # Display the confusion matrix
    print(metrics.confusion_matrix([seq_df.label[j]], y_pred))
    # Calculate the classification rate of this classifier
    #p=metrics.accuracy_score(seq_df.label[j:i], y_pred)
    #r=metrics.recall_score(seq_df.label[j:i], y_pred, average='macro')
    #f1=metrics.f1_score(seq_df.label[j:i], y_pred, average='macro')
    p1,r1,f,_=metrics.precision_recall_fscore_support([seq_df.label[j]], y_pred, average='micro')
    # print(f1,r,p)
    savescore("LSTM2.csv",p1,r1,f,le,0)
    j +=1


# %%



# %%
#seq_df.label=len(trainseq)*[1]
ss,fl,f,seuil=learn_threshold(SS,seq_df.label)
print(seuil,f)
plot_threshold(ss,fl,f,seuil)
y_pred=decision(SS,seuil)
print(y_pred)

# Calculate the classification rate of this classifier
lp=metrics.accuracy_score(seq_df.label, y_pred)
lr=metrics.recall_score(seq_df.label, y_pred, average='macro')
lf1=metrics.f1_score(seq_df.label, y_pred, average='macro')
print(lf1)


# %%
#nbseq=14
pert=.3
#nbc=5
lmu=np.mean(memory_usage((LSTMpred,(list(seq_df.sequence),))))
# Display the confusion matrix
print(metrics.confusion_matrix(seq_df.label, y_pred))
savescore("LSTM.csv",lp,lr,lf1,le,lmu)

# %% [markdown]
# ################### train LSTM model ####################

# %%
################### train LSTM model ####################
i=15
saveobj("trainseq.dictionary".format(i),trainseq)
trainseq=loadobj("trainseq.dictionary".format(i))


# %%
# summarize history for loss
history=h
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()








